* 
* ==> Audit <==
* |-----------|-------------------------------|----------|-------------|---------|-------------------------------|-------------------------------|
|  Command  |             Args              | Profile  |    User     | Version |          Start Time           |           End Time            |
|-----------|-------------------------------|----------|-------------|---------|-------------------------------|-------------------------------|
| start     |                               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 18:46:46 -05 | Wed, 15 Jun 2022 19:01:07 -05 |
| start     |                               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 19:56:35 -05 | Wed, 15 Jun 2022 19:56:58 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 19:57:01 -05 | Wed, 15 Jun 2022 19:57:06 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 20:16:18 -05 | Wed, 15 Jun 2022 20:16:42 -05 |
| stop      |                               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 20:25:37 -05 | Wed, 15 Jun 2022 20:25:48 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 20:25:54 -05 | Wed, 15 Jun 2022 20:26:14 -05 |
| stop      |                               | minikube | giovanemere | v1.25.2 | Wed, 15 Jun 2022 20:26:19 -05 | Wed, 15 Jun 2022 20:26:30 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Thu, 16 Jun 2022 10:48:27 -05 | Thu, 16 Jun 2022 10:48:55 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 06:52:48 -05 | Tue, 30 Aug 2022 06:53:13 -05 |
| stop      |                               | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 06:58:38 -05 | Tue, 30 Aug 2022 06:58:49 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 07:11:48 -05 | Tue, 30 Aug 2022 07:12:11 -05 |
| start     | --memory=1928mb               | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 18:59:18 -05 | Tue, 30 Aug 2022 18:59:42 -05 |
| addons    | enable ingress                | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 19:01:16 -05 | Tue, 30 Aug 2022 19:03:30 -05 |
| delete    |                               | minikube | giovanemere | v1.25.2 | Tue, 30 Aug 2022 19:04:26 -05 | Tue, 30 Aug 2022 19:04:29 -05 |
| start     |                               | minikube | giovanemere | v1.26.1 | 30 Aug 22 19:05 -05           |                               |
| delete    |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:07 -05           | 30 Aug 22 19:07 -05           |
| start     |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:08 -05           |                               |
| delete    |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:09 -05           | 30 Aug 22 19:09 -05           |
| start     |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:10 -05           |                               |
| start     | --vm-driver=none              | minikube | root        | v1.26.1 | 30 Aug 22 19:11 -05           |                               |
| start     |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:13 -05           |                               |
| start     | --force --driver=docker       | minikube | giovanemere | v1.26.1 | 30 Aug 22 19:14 -05           |                               |
| start     | --force --driver=docker       | minikube | root        | v1.26.1 | 30 Aug 22 19:14 -05           |                               |
| start     |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:14 -05           |                               |
| delete    |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:39 -05           | 30 Aug 22 19:39 -05           |
| start     | --force --driver=docker start | minikube | root        | v1.26.1 | 30 Aug 22 19:39 -05           |                               |
| delete    |                               | minikube | root        | v1.26.1 | 30 Aug 22 19:47 -05           | 30 Aug 22 19:47 -05           |
| start     | --force --driver=docker start | minikube | root        | v1.26.1 | 30 Aug 22 19:47 -05           | 30 Aug 22 19:59 -05           |
| addons    | enable ingress                | minikube | root        | v1.26.1 | 30 Aug 22 19:59 -05           | 30 Aug 22 20:01 -05           |
| addons    | enable dashboard              | minikube | root        | v1.26.1 | 30 Aug 22 20:01 -05           | 30 Aug 22 20:01 -05           |
| addons    | enable metrics-server         | minikube | root        | v1.26.1 | 30 Aug 22 20:01 -05           | 30 Aug 22 20:01 -05           |
| addons    | list                          | minikube | root        | v1.26.1 | 30 Aug 22 20:01 -05           | 30 Aug 22 20:01 -05           |
| addons    | list                          | minikube | root        | v1.26.1 | 30 Aug 22 20:01 -05           | 30 Aug 22 20:01 -05           |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:12 -05           |                               |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:13 -05           |                               |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:14 -05           |                               |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:15 -05           | 30 Aug 22 21:15 -05           |
| addons    | enable ingress                | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:15 -05           | 30 Aug 22 21:15 -05           |
| service   | web --url                     | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:16 -05           | 30 Aug 22 21:16 -05           |
| service   | web --url                     | minikube | giovanemere | v1.26.1 | 30 Aug 22 21:21 -05           | 30 Aug 22 21:21 -05           |
| dashboard |                               | minikube | giovanemere | v1.26.1 | 30 Aug 22 22:44 -05           |                               |
| stop      |                               | minikube | giovanemere | v1.26.1 | 30 Aug 22 23:46 -05           | 30 Aug 22 23:46 -05           |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 31 Aug 22 06:57 -05           | 31 Aug 22 06:58 -05           |
| service   | mysql --url                   | minikube | giovanemere | v1.26.1 | 31 Aug 22 07:00 -05           | 31 Aug 22 07:00 -05           |
| start     | --memory=1928mb               | minikube | giovanemere | v1.26.1 | 31 Aug 22 09:35 -05           | 31 Aug 22 09:35 -05           |
| addons    | enableingress                 | minikube | giovanemere | v1.26.1 | 31 Aug 22 09:58 -05           | 31 Aug 22 09:58 -05           |
| addons    | disable ingress               | minikube | giovanemere | v1.26.1 | 31 Aug 22 10:01 -05           | 31 Aug 22 10:01 -05           |
| addons    | enable ingresss               | minikube | giovanemere | v1.26.1 | 31 Aug 22 10:01 -05           |                               |
|-----------|-------------------------------|----------|-------------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/08/31 09:35:10
Running on machine: minikube
Binary: Built with gc go1.18.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0831 09:35:10.870167    2120 out.go:296] Setting OutFile to fd 1 ...
I0831 09:35:10.870795    2120 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
I0831 09:35:10.870798    2120 out.go:309] Setting ErrFile to fd 2...
I0831 09:35:10.870804    2120 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
I0831 09:35:10.870885    2120 root.go:333] Updating PATH: /home/giovanemere/.minikube/bin
I0831 09:35:10.872696    2120 out.go:303] Setting JSON to false
I0831 09:35:10.893396    2120 start.go:115] hostinfo: {"hostname":"minikube","uptime":517,"bootTime":1661955994,"procs":144,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"18.04","kernelVersion":"4.15.0-191-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"92f1a544-db2f-4a81-b01f-78f46cc28d74"}
I0831 09:35:10.893462    2120 start.go:125] virtualization:  
I0831 09:35:10.896993    2120 out.go:177] * minikube v1.26.1 on Ubuntu 18.04
I0831 09:35:10.909822    2120 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0831 09:35:10.910931    2120 notify.go:193] Checking for updates...
I0831 09:35:10.923936    2120 driver.go:365] Setting default libvirt URI to qemu:///system
I0831 09:35:11.117750    2120 docker.go:137] docker version: linux-20.10.17
I0831 09:35:11.117846    2120 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 09:35:11.575281    2120 info.go:265] docker info: {ID:5KND:HMJ5:DAOR:M3GN:GUH6:5X45:RSEB:GG67:VHB4:E2FB:VCH4:HJYR Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-08-31 09:35:11.1616269 -0500 -05 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.15.0-191-generic OperatingSystem:Ubuntu 18.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:11534938112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikube Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0831 09:35:11.575360    2120 docker.go:254] overlay module found
I0831 09:35:11.578966    2120 out.go:177] * Using the docker driver based on existing profile
I0831 09:35:11.581163    2120 start.go:284] selected driver: docker
I0831 09:35:11.581168    2120 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.6.0@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3 IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c MetricsServer:metrics-server/metrics-server:v0.6.1@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/giovanemere:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0831 09:35:11.581259    2120 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0831 09:35:11.581946    2120 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 09:35:11.665422    2120 info.go:265] docker info: {ID:5KND:HMJ5:DAOR:M3GN:GUH6:5X45:RSEB:GG67:VHB4:E2FB:VCH4:HJYR Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-08-31 09:35:11.6079394 -0500 -05 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.15.0-191-generic OperatingSystem:Ubuntu 18.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:11534938112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikube Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
W0831 09:35:11.665647    2120 out.go:239] ! You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I0831 09:35:11.665727    2120 cni.go:95] Creating CNI manager for ""
I0831 09:35:11.665733    2120 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0831 09:35:11.665739    2120 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.6.0@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3 IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c MetricsServer:metrics-server/metrics-server:v0.6.1@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/giovanemere:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0831 09:35:11.668163    2120 out.go:177] * Starting control plane node minikube in cluster minikube
I0831 09:35:11.670757    2120 cache.go:120] Beginning downloading kic base image for docker with docker
I0831 09:35:11.672457    2120 out.go:177] * Pulling base image ...
I0831 09:35:11.674032    2120 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0831 09:35:11.674066    2120 preload.go:148] Found local preload: /home/giovanemere/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4
I0831 09:35:11.674070    2120 cache.go:57] Caching tarball of preloaded images
I0831 09:35:11.674094    2120 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0831 09:35:11.674309    2120 preload.go:174] Found /home/giovanemere/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0831 09:35:11.674320    2120 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.3 on docker
I0831 09:35:11.674427    2120 profile.go:148] Saving config to /home/giovanemere/.minikube/profiles/minikube/config.json ...
I0831 09:35:11.702064    2120 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon, skipping pull
I0831 09:35:11.702074    2120 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 exists in daemon, skipping load
I0831 09:35:11.702080    2120 cache.go:208] Successfully downloaded all kic artifacts
I0831 09:35:11.702102    2120 start.go:371] acquiring machines lock for minikube: {Name:mkc56d556bd5bc33e0ab48298136c21e7e6773cd Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0831 09:35:11.702185    2120 start.go:375] acquired machines lock for "minikube" in 71.1Âµs
I0831 09:35:11.702197    2120 start.go:95] Skipping create...Using existing machine configuration
I0831 09:35:11.702199    2120 fix.go:55] fixHost starting: 
I0831 09:35:11.702352    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:11.741480    2120 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0831 09:35:11.741504    2120 fix.go:129] unexpected machine state, will restart: <nil>
I0831 09:35:11.749065    2120 out.go:177] * Restarting existing docker container for "minikube" ...
I0831 09:35:11.750742    2120 cli_runner.go:164] Run: docker start minikube
I0831 09:35:12.720600    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:12.755195    2120 kic.go:415] container "minikube" state is running.
I0831 09:35:12.755534    2120 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 09:35:12.790409    2120 profile.go:148] Saving config to /home/giovanemere/.minikube/profiles/minikube/config.json ...
I0831 09:35:12.790547    2120 machine.go:88] provisioning docker machine ...
I0831 09:35:12.790556    2120 ubuntu.go:169] provisioning hostname "minikube"
I0831 09:35:12.790584    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:12.818500    2120 main.go:134] libmachine: Using SSH client type: native
I0831 09:35:12.819132    2120 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0831 09:35:12.819140    2120 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0831 09:35:12.819671    2120 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:58444->127.0.0.1:49157: read: connection reset by peer
I0831 09:35:15.975246    2120 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0831 09:35:15.975337    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:16.044510    2120 main.go:134] libmachine: Using SSH client type: native
I0831 09:35:16.044924    2120 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0831 09:35:16.044958    2120 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0831 09:35:16.182250    2120 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0831 09:35:16.182265    2120 ubuntu.go:175] set auth options {CertDir:/home/giovanemere/.minikube CaCertPath:/home/giovanemere/.minikube/certs/ca.pem CaPrivateKeyPath:/home/giovanemere/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/giovanemere/.minikube/machines/server.pem ServerKeyPath:/home/giovanemere/.minikube/machines/server-key.pem ClientKeyPath:/home/giovanemere/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/giovanemere/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/giovanemere/.minikube}
I0831 09:35:16.182304    2120 ubuntu.go:177] setting up certificates
I0831 09:35:16.182312    2120 provision.go:83] configureAuth start
I0831 09:35:16.182363    2120 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 09:35:16.224130    2120 provision.go:138] copyHostCerts
I0831 09:35:16.254369    2120 exec_runner.go:144] found /home/giovanemere/.minikube/ca.pem, removing ...
I0831 09:35:16.254384    2120 exec_runner.go:207] rm: /home/giovanemere/.minikube/ca.pem
I0831 09:35:16.254451    2120 exec_runner.go:151] cp: /home/giovanemere/.minikube/certs/ca.pem --> /home/giovanemere/.minikube/ca.pem (1090 bytes)
I0831 09:35:16.255047    2120 exec_runner.go:144] found /home/giovanemere/.minikube/cert.pem, removing ...
I0831 09:35:16.255053    2120 exec_runner.go:207] rm: /home/giovanemere/.minikube/cert.pem
I0831 09:35:16.255079    2120 exec_runner.go:151] cp: /home/giovanemere/.minikube/certs/cert.pem --> /home/giovanemere/.minikube/cert.pem (1135 bytes)
I0831 09:35:16.255546    2120 exec_runner.go:144] found /home/giovanemere/.minikube/key.pem, removing ...
I0831 09:35:16.255552    2120 exec_runner.go:207] rm: /home/giovanemere/.minikube/key.pem
I0831 09:35:16.255577    2120 exec_runner.go:151] cp: /home/giovanemere/.minikube/certs/key.pem --> /home/giovanemere/.minikube/key.pem (1679 bytes)
I0831 09:35:16.256019    2120 provision.go:112] generating server cert: /home/giovanemere/.minikube/machines/server.pem ca-key=/home/giovanemere/.minikube/certs/ca.pem private-key=/home/giovanemere/.minikube/certs/ca-key.pem org=giovanemere.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0831 09:35:16.370948    2120 provision.go:172] copyRemoteCerts
I0831 09:35:16.371025    2120 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0831 09:35:16.371063    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:16.398286    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:16.482232    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0831 09:35:16.513037    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/machines/server.pem --> /etc/docker/server.pem (1216 bytes)
I0831 09:35:16.537103    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0831 09:35:16.553572    2120 provision.go:86] duration metric: configureAuth took 371.2487ms
I0831 09:35:16.553588    2120 ubuntu.go:193] setting minikube options for container-runtime
I0831 09:35:16.553809    2120 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0831 09:35:16.553843    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:16.581060    2120 main.go:134] libmachine: Using SSH client type: native
I0831 09:35:16.581160    2120 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0831 09:35:16.581166    2120 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0831 09:35:16.706373    2120 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0831 09:35:16.706386    2120 ubuntu.go:71] root file system type: overlay
I0831 09:35:16.706582    2120 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0831 09:35:16.706649    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:16.745846    2120 main.go:134] libmachine: Using SSH client type: native
I0831 09:35:16.745959    2120 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0831 09:35:16.746017    2120 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0831 09:35:16.868071    2120 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0831 09:35:16.868120    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:16.898075    2120 main.go:134] libmachine: Using SSH client type: native
I0831 09:35:16.898184    2120 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0831 09:35:16.898195    2120 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0831 09:35:17.017333    2120 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0831 09:35:17.017342    2120 machine.go:91] provisioned docker machine in 4.2267909s
I0831 09:35:17.017347    2120 start.go:307] post-start starting for "minikube" (driver="docker")
I0831 09:35:17.017351    2120 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0831 09:35:17.017391    2120 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0831 09:35:17.017421    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:17.044516    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:17.132169    2120 ssh_runner.go:195] Run: cat /etc/os-release
I0831 09:35:17.136422    2120 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0831 09:35:17.136437    2120 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0831 09:35:17.136445    2120 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0831 09:35:17.136454    2120 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I0831 09:35:17.136462    2120 filesync.go:126] Scanning /home/giovanemere/.minikube/addons for local assets ...
I0831 09:35:17.137013    2120 filesync.go:126] Scanning /home/giovanemere/.minikube/files for local assets ...
I0831 09:35:17.137410    2120 start.go:310] post-start completed in 120.0558ms
I0831 09:35:17.137483    2120 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0831 09:35:17.137518    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:17.166418    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:17.247541    2120 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0831 09:35:17.258983    2120 fix.go:57] fixHost completed within 5.5567708s
I0831 09:35:17.259004    2120 start.go:82] releasing machines lock for "minikube", held for 5.5568101s
I0831 09:35:17.259134    2120 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 09:35:17.299630    2120 ssh_runner.go:195] Run: systemctl --version
I0831 09:35:17.299667    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:17.299719    2120 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0831 09:35:17.299748    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:17.327623    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:17.335588    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:17.442786    2120 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0831 09:35:17.851376    2120 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0831 09:35:17.851412    2120 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0831 09:35:17.862730    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0831 09:35:17.874833    2120 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0831 09:35:17.956451    2120 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0831 09:35:18.036915    2120 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 09:35:18.113078    2120 ssh_runner.go:195] Run: sudo systemctl restart docker
I0831 09:35:18.531095    2120 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0831 09:35:18.608218    2120 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 09:35:18.681370    2120 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0831 09:35:18.690400    2120 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0831 09:35:18.690460    2120 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0831 09:35:18.693762    2120 start.go:471] Will wait 60s for crictl version
I0831 09:35:18.693794    2120 ssh_runner.go:195] Run: sudo crictl version
I0831 09:35:19.076138    2120 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I0831 09:35:19.076176    2120 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 09:35:19.259253    2120 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 09:35:19.296907    2120 out.go:204] * Preparing Kubernetes v1.24.3 on Docker 20.10.17 ...
I0831 09:35:19.297569    2120 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0831 09:35:19.325028    2120 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0831 09:35:19.328160    2120 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0831 09:35:19.337110    2120 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0831 09:35:19.337145    2120 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 09:35:19.367235    2120 docker.go:611] Got preloaded images: -- stdout --
mysql/mysql-operator:8.0.30-2.0.5
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/ingress-nginx/controller:<none>
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
gcr.io/google-samples/hello-app:2.0
gcr.io/google-samples/hello-app:1.0
k8s.gcr.io/metrics-server/metrics-server:<none>
mysql:5.6
<none>:<none>
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 09:35:19.367245    2120 docker.go:542] Images already preloaded, skipping extraction
I0831 09:35:19.367279    2120 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 09:35:19.397161    2120 docker.go:611] Got preloaded images: -- stdout --
mysql/mysql-operator:8.0.30-2.0.5
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/ingress-nginx/controller:<none>
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
gcr.io/google-samples/hello-app:2.0
gcr.io/google-samples/hello-app:1.0
k8s.gcr.io/metrics-server/metrics-server:<none>
mysql:5.6
<none>:<none>
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 09:35:19.397171    2120 cache_images.go:84] Images are preloaded, skipping loading
I0831 09:35:19.397205    2120 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0831 09:35:19.686050    2120 cni.go:95] Creating CNI manager for ""
I0831 09:35:19.686059    2120 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0831 09:35:19.686507    2120 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0831 09:35:19.686520    2120 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.24.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0831 09:35:19.686608    2120 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0831 09:35:19.686662    2120 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0831 09:35:19.686698    2120 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.3
I0831 09:35:19.694981    2120 binaries.go:44] Found k8s binaries, skipping transfer
I0831 09:35:19.695021    2120 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0831 09:35:19.701517    2120 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I0831 09:35:19.712643    2120 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0831 09:35:19.724200    2120 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2031 bytes)
I0831 09:35:19.736576    2120 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0831 09:35:19.739663    2120 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0831 09:35:19.747983    2120 certs.go:54] Setting up /home/giovanemere/.minikube/profiles/minikube for IP: 192.168.49.2
I0831 09:35:19.748054    2120 certs.go:182] skipping minikubeCA CA generation: /home/giovanemere/.minikube/ca.key
I0831 09:35:19.748602    2120 certs.go:182] skipping proxyClientCA CA generation: /home/giovanemere/.minikube/proxy-client-ca.key
I0831 09:35:19.748665    2120 certs.go:298] skipping minikube-user signed cert generation: /home/giovanemere/.minikube/profiles/minikube/client.key
I0831 09:35:19.749029    2120 certs.go:298] skipping minikube signed cert generation: /home/giovanemere/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0831 09:35:19.749441    2120 certs.go:298] skipping aggregator signed cert generation: /home/giovanemere/.minikube/profiles/minikube/proxy-client.key
I0831 09:35:19.749520    2120 certs.go:388] found cert: /home/giovanemere/.minikube/certs/home/giovanemere/.minikube/certs/ca-key.pem (1675 bytes)
I0831 09:35:19.749541    2120 certs.go:388] found cert: /home/giovanemere/.minikube/certs/home/giovanemere/.minikube/certs/ca.pem (1090 bytes)
I0831 09:35:19.749559    2120 certs.go:388] found cert: /home/giovanemere/.minikube/certs/home/giovanemere/.minikube/certs/cert.pem (1135 bytes)
I0831 09:35:19.749574    2120 certs.go:388] found cert: /home/giovanemere/.minikube/certs/home/giovanemere/.minikube/certs/key.pem (1679 bytes)
I0831 09:35:19.750022    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0831 09:35:19.767060    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0831 09:35:19.782763    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0831 09:35:19.799010    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0831 09:35:19.815279    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0831 09:35:19.833553    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0831 09:35:19.853800    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0831 09:35:19.874576    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0831 09:35:19.893538    2120 ssh_runner.go:362] scp /home/giovanemere/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0831 09:35:19.912594    2120 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0831 09:35:19.927537    2120 ssh_runner.go:195] Run: openssl version
I0831 09:35:19.935690    2120 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0831 09:35:19.945117    2120 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0831 09:35:19.948581    2120 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jun 16 00:00 /usr/share/ca-certificates/minikubeCA.pem
I0831 09:35:19.948618    2120 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0831 09:35:19.953849    2120 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0831 09:35:19.960625    2120 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[Dashboard:kubernetesui/dashboard:v2.6.0@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3 IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 MetricsScraper:kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c MetricsServer:metrics-server/metrics-server:v0.6.1@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/giovanemere:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0831 09:35:19.960730    2120 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0831 09:35:19.991315    2120 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0831 09:35:20.000694    2120 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I0831 09:35:20.000710    2120 kubeadm.go:626] restartCluster start
I0831 09:35:20.000758    2120 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0831 09:35:20.008171    2120 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0831 09:35:20.008689    2120 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0831 09:35:20.015715    2120 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0831 09:35:20.025745    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:20.025795    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:20.036524    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:20.237410    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:20.237530    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:20.254033    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:20.436704    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:20.436815    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:20.453766    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:20.637503    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:20.637588    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:20.658076    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:20.837666    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:20.837748    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:20.862110    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:21.037546    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:21.037614    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:21.048024    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:21.237448    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:21.237506    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:21.248013    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:21.436693    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:21.436808    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:21.444510    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:21.637171    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:21.637277    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:21.658293    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:21.837251    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:21.837320    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:21.850275    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:22.036674    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:22.036785    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:22.047963    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:22.238001    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:22.238165    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:22.260965    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:22.437423    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:22.437485    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:22.446873    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:22.637236    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:22.637291    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:22.646255    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:22.837506    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:22.837569    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:22.845677    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:23.037275    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:23.037329    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:23.045364    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:23.045371    2120 api_server.go:165] Checking apiserver status ...
I0831 09:35:23.045398    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 09:35:23.053222    2120 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 09:35:23.053233    2120 kubeadm.go:601] needs reconfigure: apiserver error: timed out waiting for the condition
I0831 09:35:23.053236    2120 kubeadm.go:1092] stopping kube-system containers ...
I0831 09:35:23.053310    2120 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0831 09:35:23.086410    2120 docker.go:443] Stopping containers: [975376a436b5 67cd94be97d7 5c84584300cf 2c581a010bdd e1a67fd4990c 7134387960b8 58f8748d2816 dc6a6951a4e0 f94b3bdf5c30 8c3dd0dc143f c355ad8627b6 6da2e376e968 fb1c1a6d0566 c4d2ecb05bb4 e95465690232 b160c99e02ec f6cd16e4a825 ec6d88d5e8db 8a67b4769343 731151878e6b 95e34978929f a2f6e2113409 b9db1929ed5f 3c7360c731e2 008ac3c01227 ab8169d49b05 4de7293276eb 0e348c789023 06d9277e8f08 1ba8c63050fa]
I0831 09:35:23.086470    2120 ssh_runner.go:195] Run: docker stop 975376a436b5 67cd94be97d7 5c84584300cf 2c581a010bdd e1a67fd4990c 7134387960b8 58f8748d2816 dc6a6951a4e0 f94b3bdf5c30 8c3dd0dc143f c355ad8627b6 6da2e376e968 fb1c1a6d0566 c4d2ecb05bb4 e95465690232 b160c99e02ec f6cd16e4a825 ec6d88d5e8db 8a67b4769343 731151878e6b 95e34978929f a2f6e2113409 b9db1929ed5f 3c7360c731e2 008ac3c01227 ab8169d49b05 4de7293276eb 0e348c789023 06d9277e8f08 1ba8c63050fa
I0831 09:35:23.148142    2120 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0831 09:35:23.158570    2120 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0831 09:35:23.166016    2120 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Aug 31 02:15 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Aug 31 11:57 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5655 Aug 31 02:15 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Aug 31 11:57 /etc/kubernetes/scheduler.conf

I0831 09:35:23.166053    2120 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0831 09:35:23.173897    2120 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0831 09:35:23.181970    2120 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0831 09:35:23.189402    2120 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0831 09:35:23.189437    2120 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0831 09:35:23.196845    2120 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0831 09:35:23.203857    2120 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0831 09:35:23.203914    2120 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0831 09:35:23.210955    2120 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0831 09:35:23.218456    2120 kubeadm.go:703] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0831 09:35:23.218465    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:23.411239    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:24.254388    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:24.909397    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:25.007651    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:25.062880    2120 api_server.go:51] waiting for apiserver process to appear ...
I0831 09:35:25.062972    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:25.577817    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:26.077759    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:26.577925    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:27.077729    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:27.577378    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:27.587737    2120 api_server.go:71] duration metric: took 2.5248547s to wait for apiserver process to appear ...
I0831 09:35:27.587772    2120 api_server.go:87] waiting for apiserver healthz status ...
I0831 09:35:27.587787    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:27.588372    2120 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0831 09:35:28.088624    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:28.088908    2120 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0831 09:35:28.589181    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:31.949334    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0831 09:35:31.949346    2120 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0831 09:35:32.089632    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:32.094786    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 09:35:32.094808    2120 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 09:35:32.589415    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:32.593660    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 09:35:32.593701    2120 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 09:35:33.089429    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:33.096435    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 09:35:33.096450    2120 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 09:35:33.589269    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:33.596766    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0831 09:35:33.619345    2120 api_server.go:140] control plane version: v1.24.3
I0831 09:35:33.619357    2120 api_server.go:130] duration metric: took 6.0315808s to wait for apiserver health ...
I0831 09:35:33.619362    2120 cni.go:95] Creating CNI manager for ""
I0831 09:35:33.619367    2120 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0831 09:35:33.619391    2120 system_pods.go:43] waiting for kube-system pods to appear ...
I0831 09:35:33.631047    2120 system_pods.go:59] 8 kube-system pods found
I0831 09:35:33.631059    2120 system_pods.go:61] "coredns-6d4b75cb6d-gbqcr" [c58bbd32-9c57-46e6-b0b1-f2d69c9f82fb] Running
I0831 09:35:33.631062    2120 system_pods.go:61] "etcd-minikube" [65ad5d99-ef89-40a7-9172-7d54581d9a06] Running
I0831 09:35:33.631065    2120 system_pods.go:61] "kube-apiserver-minikube" [e1695f2b-06b5-45bf-95be-b4d926ecc045] Running
I0831 09:35:33.631068    2120 system_pods.go:61] "kube-controller-manager-minikube" [30ba352c-2c98-4431-a9d3-969859c12103] Running
I0831 09:35:33.631070    2120 system_pods.go:61] "kube-proxy-8k8h4" [b35cd266-166e-4230-8f2e-e329d3d96ec2] Running
I0831 09:35:33.631074    2120 system_pods.go:61] "kube-scheduler-minikube" [fb039113-d133-4f15-b98c-4bcc288e32e4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0831 09:35:33.631078    2120 system_pods.go:61] "metrics-server-8595bd7d4c-hhfdk" [3c603693-6dfb-45d8-b5f7-5ef23e49ab11] Running
I0831 09:35:33.631080    2120 system_pods.go:61] "storage-provisioner" [c8a46fbb-62ec-4604-a40f-f56a39954d00] Running
I0831 09:35:33.631084    2120 system_pods.go:74] duration metric: took 11.6898ms to wait for pod list to return data ...
I0831 09:35:33.631088    2120 node_conditions.go:102] verifying NodePressure condition ...
I0831 09:35:33.634056    2120 node_conditions.go:122] node storage ephemeral capacity is 115421004Ki
I0831 09:35:33.634067    2120 node_conditions.go:123] node cpu capacity is 4
I0831 09:35:33.634074    2120 node_conditions.go:105] duration metric: took 2.9841ms to run NodePressure ...
I0831 09:35:33.634086    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0831 09:35:33.913555    2120 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0831 09:35:33.923269    2120 ops.go:34] apiserver oom_adj: -16
I0831 09:35:33.923279    2120 kubeadm.go:630] restartCluster took 13.9225645s
I0831 09:35:33.923283    2120 kubeadm.go:397] StartCluster complete in 13.9626648s
I0831 09:35:33.923294    2120 settings.go:142] acquiring lock: {Name:mk24dbcbb8edfe001f89e39ef835d0b2119106e5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 09:35:33.923373    2120 settings.go:150] Updating kubeconfig:  /home/giovanemere/.kube/config
I0831 09:35:33.924789    2120 lock.go:35] WriteFile acquiring /home/giovanemere/.kube/config: {Name:mk3a87c7bc06e9e4dd35b5a42eb139b05489ed7c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 09:35:33.945464    2120 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0831 09:35:33.945594    2120 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0831 09:35:33.945871    2120 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0831 09:35:33.945546    2120 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0831 09:35:34.096606    2120 out.go:177] * Verifying Kubernetes components...
I0831 09:35:33.945924    2120 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0831 09:35:34.096772    2120 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0831 09:35:34.096785    2120 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0831 09:35:34.096789    2120 addons.go:162] addon storage-provisioner should already be in state true
I0831 09:35:34.096870    2120 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0831 09:35:34.096885    2120 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0831 09:35:34.096888    2120 addons.go:65] Setting dashboard=true in profile "minikube"
I0831 09:35:34.096897    2120 addons.go:153] Setting addon dashboard=true in "minikube"
W0831 09:35:34.096920    2120 addons.go:162] addon dashboard should already be in state true
I0831 09:35:34.096984    2120 addons.go:65] Setting ingress=true in profile "minikube"
I0831 09:35:34.096993    2120 addons.go:153] Setting addon ingress=true in "minikube"
W0831 09:35:34.096996    2120 addons.go:162] addon ingress should already be in state true
I0831 09:35:34.097153    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.108460    2120 addons.go:65] Setting metrics-server=true in profile "minikube"
I0831 09:35:34.118735    2120 host.go:66] Checking if "minikube" exists ...
I0831 09:35:34.148418    2120 addons.go:153] Setting addon metrics-server=true in "minikube"
I0831 09:35:34.118734    2120 host.go:66] Checking if "minikube" exists ...
W0831 09:35:34.148428    2120 addons.go:162] addon metrics-server should already be in state true
I0831 09:35:34.148476    2120 host.go:66] Checking if "minikube" exists ...
I0831 09:35:34.148715    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.148718    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.148729    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.148821    2120 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0831 09:35:34.118733    2120 host.go:66] Checking if "minikube" exists ...
I0831 09:35:34.149280    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.166800    2120 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0831 09:35:34.166810    2120 addons.go:162] addon default-storageclass should already be in state true
I0831 09:35:34.166827    2120 host.go:66] Checking if "minikube" exists ...
I0831 09:35:34.167074    2120 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 09:35:34.220592    2120 out.go:177]   - Using image kubernetesui/dashboard:v2.6.0
I0831 09:35:34.219863    2120 out.go:177]   - Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0831 09:35:34.225429    2120 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0831 09:35:34.274256    2120 out.go:177]   - Using image k8s.gcr.io/metrics-server/metrics-server:v0.6.1
I0831 09:35:34.274281    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0831 09:35:34.357354    2120 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0831 09:35:34.359083    2120 out.go:177]   - Using image kubernetesui/metrics-scraper:v1.0.8
I0831 09:35:34.490689    2120 addons.go:345] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0831 09:35:34.490732    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0831 09:35:34.490800    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:34.450237    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:34.528203    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0831 09:35:34.528214    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0831 09:35:34.528270    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:34.526747    2120 out.go:177]   - Using image k8s.gcr.io/ingress-nginx/controller:v1.2.1
I0831 09:35:34.527772    2120 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0831 09:35:34.528631    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0831 09:35:34.528665    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:34.531477    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:34.542068    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:34.587026    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:34.599353    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:34.654750    2120 out.go:177]   - Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0831 09:35:34.666410    2120 addons.go:345] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0831 09:35:34.666419    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (15567 bytes)
I0831 09:35:34.666459    2120 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 09:35:34.697605    2120 addons.go:345] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0831 09:35:34.697613    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1902 bytes)
I0831 09:35:34.712247    2120 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/giovanemere/.minikube/machines/minikube/id_rsa Username:docker}
I0831 09:35:34.727541    2120 addons.go:345] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0831 09:35:34.727551    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0831 09:35:34.767017    2120 addons.go:345] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0831 09:35:34.767029    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0831 09:35:34.797070    2120 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0831 09:35:34.797130    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0831 09:35:34.797137    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0831 09:35:34.797141    2120 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 09:35:34.821553    2120 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0831 09:35:34.863976    2120 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0831 09:35:34.867232    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0831 09:35:34.867242    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0831 09:35:35.006931    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0831 09:35:35.006944    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0831 09:35:35.161867    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0831 09:35:35.161880    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I0831 09:35:35.196178    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-role.yaml
I0831 09:35:35.196192    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0831 09:35:35.354815    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0831 09:35:35.354846    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0831 09:35:35.446692    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0831 09:35:35.446706    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0831 09:35:35.546364    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0831 09:35:35.546383    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0831 09:35:35.646639    2120 addons.go:345] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0831 09:35:35.646653    2120 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0831 09:35:35.699668    2120 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0831 09:35:39.825761    2120 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (5.8801181s)
I0831 09:35:39.825864    2120 start.go:789] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0831 09:35:39.825891    2120 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (5.6770577s)
I0831 09:35:39.825961    2120 api_server.go:51] waiting for apiserver process to appear ...
I0831 09:35:39.826016    2120 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 09:35:41.259236    2120 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.4621485s)
I0831 09:35:41.259261    2120 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.4621127s)
I0831 09:35:41.259308    2120 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (6.437745s)
I0831 09:35:41.259313    2120 addons.go:383] Verifying addon metrics-server=true in "minikube"
I0831 09:35:41.423720    2120 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (6.5597164s)
I0831 09:35:41.423734    2120 addons.go:383] Verifying addon ingress=true in "minikube"
I0831 09:35:41.423846    2120 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.7241609s)
I0831 09:35:41.426072    2120 out.go:177] * Verifying ingress addon...
I0831 09:35:41.423875    2120 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.5978525s)
I0831 09:35:41.426120    2120 api_server.go:71] duration metric: took 7.4802048s to wait for apiserver process to appear ...
I0831 09:35:41.426127    2120 api_server.go:87] waiting for apiserver healthz status ...
I0831 09:35:41.426135    2120 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0831 09:35:41.428426    2120 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0831 09:35:41.445540    2120 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0831 09:35:41.445586    2120 kapi.go:86] Found 0 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0831 09:35:41.446196    2120 api_server.go:140] control plane version: v1.24.3
I0831 09:35:41.446201    2120 api_server.go:130] duration metric: took 20.0713ms to wait for apiserver health ...
I0831 09:35:41.446205    2120 system_pods.go:43] waiting for kube-system pods to appear ...
I0831 09:35:41.453763    2120 system_pods.go:59] 8 kube-system pods found
I0831 09:35:41.453779    2120 system_pods.go:61] "coredns-6d4b75cb6d-gbqcr" [c58bbd32-9c57-46e6-b0b1-f2d69c9f82fb] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0831 09:35:41.453785    2120 system_pods.go:61] "etcd-minikube" [65ad5d99-ef89-40a7-9172-7d54581d9a06] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0831 09:35:41.453789    2120 system_pods.go:61] "kube-apiserver-minikube" [e1695f2b-06b5-45bf-95be-b4d926ecc045] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0831 09:35:41.453794    2120 system_pods.go:61] "kube-controller-manager-minikube" [30ba352c-2c98-4431-a9d3-969859c12103] Running
I0831 09:35:41.453797    2120 system_pods.go:61] "kube-proxy-8k8h4" [b35cd266-166e-4230-8f2e-e329d3d96ec2] Running
I0831 09:35:41.453801    2120 system_pods.go:61] "kube-scheduler-minikube" [fb039113-d133-4f15-b98c-4bcc288e32e4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0831 09:35:41.453805    2120 system_pods.go:61] "metrics-server-8595bd7d4c-hhfdk" [3c603693-6dfb-45d8-b5f7-5ef23e49ab11] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0831 09:35:41.453808    2120 system_pods.go:61] "storage-provisioner" [c8a46fbb-62ec-4604-a40f-f56a39954d00] Running
I0831 09:35:41.453812    2120 system_pods.go:74] duration metric: took 7.6036ms to wait for pod list to return data ...
I0831 09:35:41.453818    2120 kubeadm.go:572] duration metric: took 7.5079044s to wait for : map[apiserver:true system_pods:true] ...
I0831 09:35:41.453826    2120 node_conditions.go:102] verifying NodePressure condition ...
I0831 09:35:41.457457    2120 node_conditions.go:122] node storage ephemeral capacity is 115421004Ki
I0831 09:35:41.457466    2120 node_conditions.go:123] node cpu capacity is 4
I0831 09:35:41.457472    2120 node_conditions.go:105] duration metric: took 3.6439ms to run NodePressure ...
I0831 09:35:41.457479    2120 start.go:216] waiting for startup goroutines ...
I0831 09:35:50.449037    2120 kapi.go:86] Found 2 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0831 09:35:50.449044    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:50.949238    2120 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0831 09:35:50.949245    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:51.449712    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:51.949790    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:52.465228    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:52.949264    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:53.451608    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:53.953264    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:54.450684    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:54.952145    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:55.450485    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:55.951191    2120 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0831 09:35:56.450293    2120 kapi.go:108] duration metric: took 15.0218627s to wait for app.kubernetes.io/name=ingress-nginx ...
I0831 09:35:56.452814    2120 out.go:177] * Enabled addons: default-storageclass, storage-provisioner, metrics-server, dashboard, ingress
I0831 09:35:56.455013    2120 addons.go:414] enableAddons completed in 22.5090936s
I0831 09:35:56.491081    2120 start.go:506] kubectl: 1.24.1, cluster: 1.24.3 (minor skew: 0)
I0831 09:35:56.493725    2120 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Wed 2022-08-31 14:35:13 UTC, end at Wed 2022-08-31 15:02:01 UTC. --
Aug 31 14:35:13 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.117295400Z" level=info msg="Starting up"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.159820200Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.159869600Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.160343800Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.160402100Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.182658400Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.182697800Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.182715800Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.182727400Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.251841100Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.374853800Z" level=warning msg="Your kernel does not support swap memory limit"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.374915500Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.375488900Z" level=info msg="Loading containers: start."
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.738408900Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.831951400Z" level=info msg="Loading containers: done."
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.890783600Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.891232800Z" level=info msg="Daemon has completed initialization"
Aug 31 14:35:14 minikube systemd[1]: Started Docker Application Container Engine.
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.916536500Z" level=info msg="API listen on [::]:2376"
Aug 31 14:35:14 minikube dockerd[244]: time="2022-08-31T14:35:14.919913600Z" level=info msg="API listen on /var/run/docker.sock"
Aug 31 14:35:18 minikube systemd[1]: Stopping Docker Application Container Engine...
Aug 31 14:35:18 minikube dockerd[244]: time="2022-08-31T14:35:18.121474500Z" level=info msg="Processing signal 'terminated'"
Aug 31 14:35:18 minikube dockerd[244]: time="2022-08-31T14:35:18.163063700Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Aug 31 14:35:18 minikube dockerd[244]: time="2022-08-31T14:35:18.163673200Z" level=info msg="Daemon shutdown complete"
Aug 31 14:35:18 minikube dockerd[244]: time="2022-08-31T14:35:18.163761400Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Aug 31 14:35:18 minikube systemd[1]: docker.service: Succeeded.
Aug 31 14:35:18 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 31 14:35:18 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.209251300Z" level=info msg="Starting up"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.216969200Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.216997700Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.217019600Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.217033700Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.218558800Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.218596200Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.218616300Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.218622900Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.244338200Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.254635500Z" level=warning msg="Your kernel does not support swap memory limit"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.254696100Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.254995000Z" level=info msg="Loading containers: start."
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.430423700Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.489928100Z" level=info msg="Loading containers: done."
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.520080000Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.520172800Z" level=info msg="Daemon has completed initialization"
Aug 31 14:35:18 minikube systemd[1]: Started Docker Application Container Engine.
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.536431700Z" level=info msg="API listen on [::]:2376"
Aug 31 14:35:18 minikube dockerd[564]: time="2022-08-31T14:35:18.540288900Z" level=info msg="API listen on /var/run/docker.sock"
Aug 31 14:35:34 minikube dockerd[564]: time="2022-08-31T14:35:34.663015500Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Aug 31 14:35:52 minikube dockerd[564]: time="2022-08-31T14:35:52.493599800Z" level=info msg="ignoring event" container=bb5d506b06f0534ac83847d9b78a3c3c00cc726016ac977347dc863057414570 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:35:52 minikube dockerd[564]: time="2022-08-31T14:35:52.797211000Z" level=info msg="ignoring event" container=1673f68524f54e44a365b275142464eb6ba3f3f794d79c4cf0476b31d36153ff module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:35:54 minikube dockerd[564]: time="2022-08-31T14:35:54.019585700Z" level=info msg="ignoring event" container=354baa2c915e23e9f8998890d7faf445e828ed0947031dc124e1b527b99e1a18 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:35:54 minikube dockerd[564]: time="2022-08-31T14:35:54.029538900Z" level=info msg="ignoring event" container=e5032dbf45e40cd889b5684ce65836c19f906ad62fe56bcad4c586fb5be63f2a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:36:05 minikube dockerd[564]: time="2022-08-31T14:36:05.492140900Z" level=info msg="ignoring event" container=e388a00ffca4b7e0378a6d53d2848d48d4fec635a6e575490f9293aef4a2715b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:38:58 minikube dockerd[564]: time="2022-08-31T14:38:58.423816300Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155
Aug 31 14:38:58 minikube dockerd[564]: time="2022-08-31T14:38:58.523661300Z" level=info msg="ignoring event" container=2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 14:38:58 minikube dockerd[564]: time="2022-08-31T14:38:58.650498500Z" level=info msg="ignoring event" container=02bec483fd98ce674ac5b2a4e8c5345e46688f0132ba90369fea6cea48e3024b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID
4c7af58efd526       6e38f40d628db       25 minutes ago      Running             storage-provisioner         7                   46400dcf13dd8
35c9f6417194c       1042d9e0d8fcc       26 minutes ago      Running             kubernetes-dashboard        4                   130cf3d649822
ea4a6763c27df       e57a417f15d36       26 minutes ago      Running             metrics-server              4                   d5185d1081055
07f2d63258732       a4ca41631cc7a       26 minutes ago      Running             coredns                     3                   a2a7f8df9a54a
e388a00ffca4b       6e38f40d628db       26 minutes ago      Exited              storage-provisioner         6                   46400dcf13dd8
16a9df4c76737       2ae1ba6417cbc       26 minutes ago      Running             kube-proxy                  3                   843082d00024b
af506c3c2d2f2       115053965e86b       26 minutes ago      Running             dashboard-metrics-scraper   3                   5f37aae6f82e7
36b75e5396a41       3a5aa3a515f5d       26 minutes ago      Running             kube-scheduler              3                   e9e35764241bb
f54d0f48a343d       aebe758cef4cd       26 minutes ago      Running             etcd                        3                   1bbd9f86697f0
8234360ce1c7a       d521dd763e2e3       26 minutes ago      Running             kube-apiserver              3                   c8e644510224f
371ddc846c711       586c112956dfc       26 minutes ago      Running             kube-controller-manager     3                   db06a424e1971
26817c5284916       1042d9e0d8fcc       3 hours ago         Exited              kubernetes-dashboard        3                   fef44cd1749da
67cd94be97d76       e57a417f15d36       3 hours ago         Exited              metrics-server              3                   8c3dd0dc143f0
5c84584300cf7       a4ca41631cc7a       3 hours ago         Exited              coredns                     2                   58f8748d28160
e1a67fd4990c0       2ae1ba6417cbc       3 hours ago         Exited              kube-proxy                  2                   dc6a6951a4e0d
41f1e474b88c2       115053965e86b       3 hours ago         Exited              dashboard-metrics-scraper   2                   4eb19e76ffe29
c355ad8627b65       3a5aa3a515f5d       3 hours ago         Exited              kube-scheduler              2                   e954656902327
6da2e376e9686       d521dd763e2e3       3 hours ago         Exited              kube-apiserver              2                   b160c99e02ecc
fb1c1a6d05661       aebe758cef4cd       3 hours ago         Exited              etcd                        2                   f6cd16e4a825a
c4d2ecb05bb48       586c112956dfc       3 hours ago         Exited              kube-controller-manager     2                   ec6d88d5e8db4

* 
* ==> coredns [07f2d6325873] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [5c84584300cf] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=62e108c3dfdec8029a890ad6d8ef96b6461426dc
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_08_30T19_59_03_0700
                    minikube.k8s.io/version=v1.26.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 31 Aug 2022 00:59:02 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 31 Aug 2022 15:01:54 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 31 Aug 2022 15:01:34 +0000   Wed, 31 Aug 2022 00:59:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 31 Aug 2022 15:01:34 +0000   Wed, 31 Aug 2022 00:59:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 31 Aug 2022 15:01:34 +0000   Wed, 31 Aug 2022 00:59:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 31 Aug 2022 15:01:34 +0000   Wed, 31 Aug 2022 00:59:13 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  115421004Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11264588Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  115421004Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11264588Ki
  pods:               110
System Info:
  Machine ID:                 4c192b04687c403f8fbb9bc7975b21b3
  System UUID:                b30b690f-cdae-4898-b10e-2e7d0ce24c0c
  Boot ID:                    a64aa74a-54a9-4108-9861-0136e34c2f79
  Kernel Version:             4.15.0-191-generic
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6d4b75cb6d-gbqcr                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     14h
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         14h
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kube-system                 kube-proxy-8k8h4                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kube-system                 metrics-server-8595bd7d4c-hhfdk               100m (2%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         14h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kubernetes-dashboard        dashboard-metrics-scraper-78dbd9dbf5-29pg2    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
  kubernetes-dashboard        kubernetes-dashboard-5fd5574d9f-kccl7         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             370Mi (3%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 26m                kube-proxy       
  Normal  Starting                 26m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           26m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.012728] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[  +2.048485] piix4_smbus 0000:00:07.3: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
[  +0.432558] psmouse serio1: trackpoint: failed to get extended button data, assuming 3 buttons
[  +3.884324] print_req_error: I/O error, dev fd0, sector 0
[  +1.475805] print_req_error: I/O error, dev fd0, sector 0
[  +8.556376] new mount options do not match the existing superblock, will be ignored
[  +2.857703] kauditd_printk_skb: 18 callbacks suppressed
[Aug31 14:35] ------------[ cut here ]------------
[  +0.000007] rq->tmp_alone_branch != &rq->leaf_cfs_rq_list
[  +0.000007] WARNING: CPU: 0 PID: 0 at /build/linux-sOfoQi/linux-4.15.0/kernel/sched/fair.c:393 enqueue_task_fair+0x959/0x9e0
[  +0.000000] Modules linked in: ip6table_mangle ip6t_MASQUERADE nf_nat_masquerade_ipv6 ip6table_nat nf_nat_ipv6 iptable_mangle xt_mark nf_tables xt_nat veth ipt_MASQUERADE nf_nat_masquerade_ipv4 nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo iptable_nat nf_nat_ipv4 br_netfilter bridge stp llc aufs overlay ip6t_REJECT nf_reject_ipv6 nf_log_ipv6 xt_hl ip6t_rt nf_conntrack_ipv6 nf_defrag_ipv6 ipt_REJECT nf_reject_ipv4 xt_comment nf_log_ipv4 nf_log_common xt_LOG xt_multiport xt_limit input_leds xt_tcpudp xt_addrtype hv_balloon serio_raw intel_rapl_perf hyperv_fb joydev mac_hid nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack ip6table_filter ip6_tables nf_conntrack_netbios_ns nf_conntrack_broadcast nf_nat_ftp nf_nat sch_fq_codel nf_conntrack_ftp nf_conntrack iptable_filter ib_iser rdma_cm iw_cm ib_cm
[  +0.000026]  ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel hid_generic pcbc hid_hyperv hv_utils hv_netvsc ptp pps_core hv_storvsc hid scsi_transport_fc hyperv_keyboard aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse floppy hv_vmbus i2c_piix4 pata_acpi
[  +0.000016] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.15.0-191-generic #202-Ubuntu
[  +0.000001] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS 090008  12/07/2018
[  +0.000001] RIP: 0010:enqueue_task_fair+0x959/0x9e0
[  +0.000001] RSP: 0018:ffff97a7c7c03da0 EFLAGS: 00010086
[  +0.000001] RAX: 0000000000000000 RBX: ffff97a7b261f600 RCX: 0000000000000000
[  +0.000000] RDX: 000000000000002d RSI: ffffffffb6d68c4d RDI: 0000000000000046
[  +0.000001] RBP: ffff97a7c7c03df0 R08: 0000000000000002 R09: ffffffffb6d68c20
[  +0.000000] R10: ffff97a7c7c03cb8 R11: 000001145b335d8a R12: 0000000000000000
[  +0.000001] R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000072
[  +0.000000] FS:  0000000000000000(0000) GS:ffff97a7c7c00000(0000) knlGS:0000000000000000
[  +0.000001] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 000000c000606f80 CR3: 000000006040a001 CR4: 00000000003606f0
[  +0.000001] Call Trace:
[  +0.000001]  <IRQ>
[  +0.000003]  ? blk_run_queue+0x3f/0x50
[  +0.000001]  activate_task+0x54/0xc0
[  +0.000001]  ttwu_do_activate+0x49/0x80
[  +0.000001]  try_to_wake_up+0x1db/0x4b0
[  +0.000001]  ? __hrtimer_init+0xc0/0xc0
[  +0.000001]  wake_up_process+0x15/0x20
[  +0.000000]  hrtimer_wakeup+0x22/0x30
[  +0.000001]  __hrtimer_run_queues+0xdf/0x230
[  +0.000001]  hrtimer_interrupt+0xa0/0x1d0
[  +0.000004]  vmbus_isr+0x180/0x290 [hv_vmbus]
[  +0.000001]  hyperv_vector_handler+0x3f/0x80
[  +0.000003]  hyperv_callback_vector+0x90/0xa0
[  +0.000000]  </IRQ>
[  +0.000001] RIP: 0010:native_safe_halt+0x12/0x20
[  +0.000001] RSP: 0018:ffffffffb6803e28 EFLAGS: 00000246 ORIG_RAX: ffffffffffffff0c
[  +0.000000] RAX: ffffffffb5dd03d0 RBX: 0000000000000000 RCX: 0000000000000000
[  +0.000001] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[  +0.000000] RBP: ffffffffb6803e28 R08: ffff97a7c7c22080 R09: 0000000000000000
[  +0.000000] R10: 0000000000000000 R11: 7fffffffffffffff R12: 0000000000000000
[  +0.000001] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001]  ? __sched_text_end+0x1/0x1
[  +0.000001]  default_idle+0x20/0x100
[  +0.000002]  arch_cpu_idle+0x15/0x20
[  +0.000000]  default_idle_call+0x23/0x30
[  +0.000001]  do_idle+0x172/0x1f0
[  +0.000002]  cpu_startup_entry+0x73/0x80
[  +0.000001]  rest_init+0xae/0xb0
[  +0.000001]  start_kernel+0x4dc/0x500
[  +0.000001]  x86_64_start_reservations+0x24/0x26
[  +0.000001]  x86_64_start_kernel+0x74/0x77
[  +0.000001]  secondary_startup_64+0xa5/0xb0
[  +0.000001] Code: 89 b2 60 09 00 00 e9 69 f7 ff ff 80 3d e2 6d 54 01 00 0f 85 74 f7 ff ff 48 c7 c7 40 4a 4d b6 c6 05 ce 6d 54 01 01 e8 e7 45 fc ff <0f> 0b e9 5a f7 ff ff 80 3d bd 6d 54 01 00 0f 85 ce fd ff ff 48 
[  +0.000009] ---[ end trace b5368c3e1c20c5d7 ]---

* 
* ==> etcd [f54d0f48a343] <==
* {"level":"info","ts":"2022-08-31T14:35:28.273Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-08-31T14:35:28.274Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2022-08-31T14:35:28.274Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-08-31T14:35:28.275Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-08-31T14:35:28.420Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-08-31T14:35:28.445Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.3","git-sha":"0452feec7","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-08-31T14:35:28.478Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"32.9739ms"}
{"level":"info","ts":"2022-08-31T14:35:28.975Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.9 kB"}
{"level":"info","ts":"2022-08-31T14:35:28.975Z","caller":"etcdserver/server.go:521","msg":"recovered v3 backend from snapshot","backend-size-bytes":3928064,"backend-size":"3.9 MB","backend-size-in-use-bytes":2236416,"backend-size-in-use":"2.2 MB"}
{"level":"info","ts":"2022-08-31T14:35:29.200Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":24645}
{"level":"info","ts":"2022-08-31T14:35:29.200Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-08-31T14:35:29.200Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2022-08-31T14:35:29.200Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 24645, applied: 20002, lastindex: 24645, lastterm: 4]"}
{"level":"info","ts":"2022-08-31T14:35:29.201Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-08-31T14:35:29.201Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-08-31T14:35:29.201Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-08-31T14:35:29.246Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-08-31T14:35:29.248Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":19560}
{"level":"info","ts":"2022-08-31T14:35:29.249Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":20026}
{"level":"info","ts":"2022-08-31T14:35:29.252Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-08-31T14:35:29.255Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2022-08-31T14:35:29.256Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-08-31T14:35:29.256Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.3","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2022-08-31T14:35:29.256Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-08-31T14:35:29.271Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-08-31T14:35:29.271Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-08-31T14:35:29.271Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-08-31T14:35:29.271Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-08-31T14:35:29.276Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-08-31T14:35:29.832Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-08-31T14:35:29.833Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-08-31T14:35:29.833Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-08-31T14:35:29.843Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-08-31T14:35:29.843Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-08-31T14:45:29.891Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20533}
{"level":"info","ts":"2022-08-31T14:45:29.906Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20533,"took":"15.2053ms"}
{"level":"info","ts":"2022-08-31T14:50:29.898Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20743}
{"level":"info","ts":"2022-08-31T14:50:29.898Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20743,"took":"602.8Âµs"}
{"level":"info","ts":"2022-08-31T14:55:29.931Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20954}
{"level":"info","ts":"2022-08-31T14:55:29.933Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20954,"took":"654.8Âµs"}
{"level":"info","ts":"2022-08-31T15:00:29.947Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21163}
{"level":"info","ts":"2022-08-31T15:00:29.949Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":21163,"took":"1.1895ms"}

* 
* ==> etcd [fb1c1a6d0566] <==
* {"level":"info","ts":"2022-08-31T12:18:01.339Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13444,"took":"927.9Âµs"}
{"level":"info","ts":"2022-08-31T12:23:01.374Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13700}
{"level":"info","ts":"2022-08-31T12:23:01.376Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13700,"took":"1.0305ms"}
{"level":"info","ts":"2022-08-31T12:28:01.408Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13955}
{"level":"info","ts":"2022-08-31T12:28:01.409Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13955,"took":"892Âµs"}
{"level":"info","ts":"2022-08-31T12:33:01.449Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14210}
{"level":"info","ts":"2022-08-31T12:33:01.451Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14210,"took":"974.8Âµs"}
{"level":"info","ts":"2022-08-31T12:38:01.460Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14466}
{"level":"info","ts":"2022-08-31T12:38:01.461Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14466,"took":"1.103ms"}
{"level":"info","ts":"2022-08-31T12:43:01.502Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14721}
{"level":"info","ts":"2022-08-31T12:43:01.505Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14721,"took":"1.4746ms"}
{"level":"info","ts":"2022-08-31T12:48:01.544Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14976}
{"level":"info","ts":"2022-08-31T12:48:01.545Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14976,"took":"1.039ms"}
{"level":"info","ts":"2022-08-31T12:53:01.549Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15231}
{"level":"info","ts":"2022-08-31T12:53:01.551Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15231,"took":"1.5215ms"}
{"level":"info","ts":"2022-08-31T12:58:01.562Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15486}
{"level":"info","ts":"2022-08-31T12:58:01.563Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15486,"took":"795.3Âµs"}
{"level":"info","ts":"2022-08-31T13:03:01.589Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15741}
{"level":"info","ts":"2022-08-31T13:03:01.591Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15741,"took":"1.6008ms"}
{"level":"info","ts":"2022-08-31T13:08:01.630Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16025}
{"level":"info","ts":"2022-08-31T13:08:01.635Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16025,"took":"2.9998ms"}
{"level":"info","ts":"2022-08-31T13:08:18.104Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-08-31T13:08:18.111Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2022-08-31T13:08:18.111Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2022-08-31T13:13:01.638Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16279}
{"level":"info","ts":"2022-08-31T13:13:01.639Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16279,"took":"724.7Âµs"}
{"level":"info","ts":"2022-08-31T13:18:01.675Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16536}
{"level":"info","ts":"2022-08-31T13:18:01.676Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16536,"took":"563.2Âµs"}
{"level":"info","ts":"2022-08-31T13:23:01.711Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16792}
{"level":"info","ts":"2022-08-31T13:23:01.711Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16792,"took":"357Âµs"}
{"level":"info","ts":"2022-08-31T13:28:01.716Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17048}
{"level":"info","ts":"2022-08-31T13:28:01.716Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17048,"took":"528.8Âµs"}
{"level":"info","ts":"2022-08-31T13:33:01.752Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17304}
{"level":"info","ts":"2022-08-31T13:33:01.754Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17304,"took":"1.107ms"}
{"level":"info","ts":"2022-08-31T13:38:01.786Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17559}
{"level":"info","ts":"2022-08-31T13:38:01.786Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17559,"took":"387.8Âµs"}
{"level":"info","ts":"2022-08-31T13:43:01.823Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17814}
{"level":"info","ts":"2022-08-31T13:43:01.825Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17814,"took":"1.0953ms"}
{"level":"info","ts":"2022-08-31T13:48:01.859Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18070}
{"level":"info","ts":"2022-08-31T13:48:01.860Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18070,"took":"557.8Âµs"}
{"level":"info","ts":"2022-08-31T13:53:01.865Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18326}
{"level":"info","ts":"2022-08-31T13:53:01.866Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18326,"took":"371.1Âµs"}
{"level":"info","ts":"2022-08-31T13:58:01.896Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18582}
{"level":"info","ts":"2022-08-31T13:58:01.897Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18582,"took":"366Âµs"}
{"level":"info","ts":"2022-08-31T14:03:01.903Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18837}
{"level":"info","ts":"2022-08-31T14:03:01.903Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18837,"took":"407Âµs"}
{"level":"info","ts":"2022-08-31T14:08:01.940Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19092}
{"level":"info","ts":"2022-08-31T14:08:01.941Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19092,"took":"719.6Âµs"}
{"level":"info","ts":"2022-08-31T14:13:01.977Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19332}
{"level":"info","ts":"2022-08-31T14:13:01.977Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19332,"took":"607.4Âµs"}
{"level":"info","ts":"2022-08-31T14:18:02.012Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19560}
{"level":"info","ts":"2022-08-31T14:18:02.013Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19560,"took":"338.4Âµs"}
{"level":"info","ts":"2022-08-31T14:22:01.309Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-08-31T14:22:01.317Z","caller":"embed/etcd.go:368","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-08-31T14:22:01.366Z","caller":"etcdserver/server.go:1453","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
WARNING: 2022/08/31 14:22:01 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
WARNING: 2022/08/31 14:22:01 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-08-31T14:22:01.370Z","caller":"embed/etcd.go:563","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-08-31T14:22:01.372Z","caller":"embed/etcd.go:568","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-08-31T14:22:01.372Z","caller":"embed/etcd.go:370","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  15:02:02 up 35 min,  0 users,  load average: 0.22, 0.43, 0.35
Linux minikube 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [6da2e376e968] <==
* W0831 14:22:09.131162       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.680889       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.716036       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.738342       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.773815       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.775025       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.825213       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.849803       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.887580       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.890165       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.912120       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.927735       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.938295       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:09.991065       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.119608       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.145877       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.174198       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.178788       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.275024       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.286679       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.293675       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.355720       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.369612       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.375643       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.388562       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.399021       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.421479       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.425515       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.446616       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.452752       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.502421       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.516978       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.589157       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.596283       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.604862       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.669241       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.741821       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.755685       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.821219       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.829195       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.845783       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.861789       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.921276       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.939743       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:10.947444       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.005971       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.020543       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.030108       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.071550       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.088166       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.103988       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.104052       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.106377       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.116913       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.121907       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.150630       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.161400       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.161417       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.225060       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0831 14:22:11.240513       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-apiserver [8234360ce1c7] <==
* I0831 14:35:31.901356       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0831 14:35:31.901701       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0831 14:35:31.908550       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0831 14:35:31.908614       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0831 14:35:31.908793       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0831 14:35:31.908826       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0831 14:35:31.908846       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0831 14:35:31.912180       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0831 14:35:31.912197       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I0831 14:35:31.922048       1 controller.go:85] Starting OpenAPI controller
I0831 14:35:31.922136       1 controller.go:85] Starting OpenAPI V3 controller
I0831 14:35:31.922175       1 naming_controller.go:291] Starting NamingConditionController
I0831 14:35:31.922203       1 establishing_controller.go:76] Starting EstablishingController
I0831 14:35:31.922213       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0831 14:35:31.922222       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0831 14:35:31.922237       1 crd_finalizer.go:266] Starting CRDFinalizer
E0831 14:35:31.955892       1 controller.go:169] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0831 14:35:32.030375       1 controller.go:611] quota admission added evaluator for: namespaces
I0831 14:35:32.066686       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0831 14:35:32.073093       1 shared_informer.go:262] Caches are synced for node_authorizer
I0831 14:35:32.086599       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I0831 14:35:32.099147       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0831 14:35:32.101039       1 cache.go:39] Caches are synced for autoregister controller
I0831 14:35:32.108802       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0831 14:35:32.108871       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0831 14:35:32.113962       1 shared_informer.go:262] Caches are synced for crd-autoregister
I0831 14:35:32.725611       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0831 14:35:32.911225       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0831 14:35:33.760635       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0831 14:35:33.781728       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0831 14:35:33.838532       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0831 14:35:33.875302       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0831 14:35:33.889350       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0831 14:35:36.384897       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
E0831 14:35:37.103089       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: i/o timeout
E0831 14:35:37.116454       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.117055       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.126997       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.168258       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.190476       1 storage.go:461] Address {172.17.0.2  0xc00cffeb50 0xc0008b1ab0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 172.17.0.2 (kubernetes-dashboard/dashboard-metrics-scraper-78dbd9dbf5-29pg2))
E0831 14:35:37.190589       1 storage.go:471] Failed to find a valid address, skipping subset: &{[{172.17.0.2  0xc00cffeb50 0xc0008b1ab0}] [] [{ 8000 TCP <nil>}]}
E0831 14:35:37.249795       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.411985       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:37.734016       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:38.375671       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:39.658645       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
I0831 14:35:41.370898       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.105.83.39]
I0831 14:35:41.382213       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.100.71.94]
I0831 14:35:41.397894       1 controller.go:611] quota admission added evaluator for: jobs.batch
E0831 14:35:42.219831       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
E0831 14:35:47.340499       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.115.15:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.115.15:443: connect: connection refused
I0831 14:35:50.409491       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0831 14:35:50.460426       1 controller.go:611] quota admission added evaluator for: endpoints
I0831 14:35:50.510193       1 controller.go:611] quota admission added evaluator for: replicasets.apps
E0831 14:36:03.145613       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.108.115.15:443: i/o timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0831 14:36:03.145657       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0831 14:36:03.145683       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0831 14:36:03.146912       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0831 14:51:04.002923       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-controller-manager [371ddc846c71] <==
* I0831 14:35:49.980285       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0831 14:35:49.980719       1 shared_informer.go:262] Caches are synced for node
I0831 14:35:49.980873       1 range_allocator.go:173] Starting range CIDR allocator
I0831 14:35:49.980898       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0831 14:35:49.980947       1 shared_informer.go:262] Caches are synced for cidrallocator
I0831 14:35:49.981178       1 shared_informer.go:262] Caches are synced for daemon sets
I0831 14:35:49.985550       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0831 14:35:49.989026       1 shared_informer.go:262] Caches are synced for attach detach
I0831 14:35:49.989182       1 shared_informer.go:262] Caches are synced for taint
I0831 14:35:49.989306       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0831 14:35:49.989322       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
W0831 14:35:49.989431       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0831 14:35:49.989533       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I0831 14:35:49.989888       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0831 14:35:49.992671       1 shared_informer.go:262] Caches are synced for expand
I0831 14:35:49.992998       1 shared_informer.go:262] Caches are synced for ephemeral
I0831 14:35:49.999473       1 shared_informer.go:262] Caches are synced for endpoint
I0831 14:35:49.999504       1 shared_informer.go:262] Caches are synced for PV protection
I0831 14:35:50.000779       1 shared_informer.go:262] Caches are synced for TTL after finished
I0831 14:35:50.006829       1 shared_informer.go:262] Caches are synced for deployment
I0831 14:35:50.008352       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0831 14:35:50.017790       1 shared_informer.go:262] Caches are synced for HPA
I0831 14:35:50.020103       1 shared_informer.go:262] Caches are synced for stateful set
I0831 14:35:50.021559       1 shared_informer.go:262] Caches are synced for cronjob
I0831 14:35:50.031231       1 shared_informer.go:262] Caches are synced for disruption
I0831 14:35:50.031345       1 disruption.go:371] Sending events to api server.
I0831 14:35:50.031262       1 shared_informer.go:262] Caches are synced for GC
I0831 14:35:50.050128       1 shared_informer.go:262] Caches are synced for PVC protection
I0831 14:35:50.055247       1 shared_informer.go:262] Caches are synced for persistent volume
I0831 14:35:50.122367       1 shared_informer.go:262] Caches are synced for resource quota
I0831 14:35:50.167587       1 shared_informer.go:262] Caches are synced for resource quota
I0831 14:35:50.361816       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-cq75n"
I0831 14:35:50.362581       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:50.364513       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:50.364955       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-phgst"
I0831 14:35:50.386619       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:50.386714       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:50.387882       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:50.387932       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:50.404145       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:50.412225       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:50.515347       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-755dfbfc65 to 1"
I0831 14:35:50.562559       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-755dfbfc65" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-755dfbfc65-5lkhx"
I0831 14:35:50.584612       1 shared_informer.go:262] Caches are synced for garbage collector
I0831 14:35:50.584908       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0831 14:35:50.644967       1 shared_informer.go:262] Caches are synced for garbage collector
I0831 14:35:52.802795       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:52.866558       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:54.920877       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:55.215170       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:35:55.922528       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0831 14:35:55.936160       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:35:56.216643       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0831 14:35:56.220349       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 14:38:57.479388       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
E0831 14:38:57.479669       1 tracking_utils.go:109] "deleting tracking annotation UID expectations" err="couldn't create key for object ingress-nginx/ingress-nginx-admission-create: could not find key for obj \"ingress-nginx/ingress-nginx-admission-create\"" job="ingress-nginx/ingress-nginx-admission-create"
I0831 14:38:57.483207       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E0831 14:38:57.483402       1 tracking_utils.go:109] "deleting tracking annotation UID expectations" err="couldn't create key for object ingress-nginx/ingress-nginx-admission-patch: could not find key for obj \"ingress-nginx/ingress-nginx-admission-patch\"" job="ingress-nginx/ingress-nginx-admission-patch"
I0831 14:39:02.804956       1 namespace_controller.go:185] Namespace has been deleted ingress-nginx
I0831 14:39:15.050690       1 namespace_controller.go:185] Namespace has been deleted ingress

* 
* ==> kube-controller-manager [c4d2ecb05bb4] <==
* I0831 11:58:21.506105       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0831 11:58:21.509720       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
E0831 11:58:21.513134       1 memcache.go:206] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0831 11:58:21.514739       1 memcache.go:104] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0831 11:58:21.516708       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0831 11:58:21.518060       1 shared_informer.go:262] Caches are synced for crt configmap
I0831 11:58:21.519406       1 shared_informer.go:262] Caches are synced for cronjob
I0831 11:58:21.522503       1 shared_informer.go:262] Caches are synced for TTL
I0831 11:58:21.524899       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0831 11:58:21.528494       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0831 11:58:21.544180       1 shared_informer.go:262] Caches are synced for job
I0831 11:58:21.544760       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0831 11:58:21.545462       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0831 11:58:21.565956       1 shared_informer.go:262] Caches are synced for taint
I0831 11:58:21.566048       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
W0831 11:58:21.566196       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0831 11:58:21.566243       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I0831 11:58:21.566925       1 shared_informer.go:262] Caches are synced for GC
I0831 11:58:21.567051       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0831 11:58:21.567170       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0831 11:58:21.567308       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0831 11:58:21.567335       1 shared_informer.go:262] Caches are synced for node
I0831 11:58:21.567341       1 shared_informer.go:262] Caches are synced for PV protection
I0831 11:58:21.567404       1 range_allocator.go:173] Starting range CIDR allocator
I0831 11:58:21.567426       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0831 11:58:21.567453       1 shared_informer.go:262] Caches are synced for cidrallocator
I0831 11:58:21.570878       1 shared_informer.go:262] Caches are synced for endpoint
I0831 11:58:21.572846       1 shared_informer.go:262] Caches are synced for service account
I0831 11:58:21.573056       1 shared_informer.go:262] Caches are synced for namespace
I0831 11:58:21.584992       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0831 11:58:21.586068       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0831 11:58:21.586109       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0831 11:58:21.586166       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0831 11:58:21.590051       1 shared_informer.go:262] Caches are synced for TTL after finished
I0831 11:58:21.590989       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0831 11:58:21.595315       1 shared_informer.go:262] Caches are synced for ReplicationController
I0831 11:58:21.616025       1 shared_informer.go:262] Caches are synced for deployment
I0831 11:58:21.666391       1 shared_informer.go:262] Caches are synced for persistent volume
I0831 11:58:21.669039       1 shared_informer.go:262] Caches are synced for ephemeral
I0831 11:58:21.683960       1 shared_informer.go:262] Caches are synced for disruption
I0831 11:58:21.684001       1 disruption.go:371] Sending events to api server.
I0831 11:58:21.713414       1 shared_informer.go:262] Caches are synced for expand
I0831 11:58:21.713792       1 shared_informer.go:262] Caches are synced for PVC protection
I0831 11:58:21.716203       1 shared_informer.go:262] Caches are synced for attach detach
I0831 11:58:21.770749       1 shared_informer.go:262] Caches are synced for HPA
I0831 11:58:21.775161       1 shared_informer.go:262] Caches are synced for daemon sets
I0831 11:58:21.790025       1 shared_informer.go:262] Caches are synced for resource quota
I0831 11:58:21.803641       1 shared_informer.go:262] Caches are synced for resource quota
I0831 11:58:21.807196       1 shared_informer.go:262] Caches are synced for stateful set
I0831 11:58:22.216711       1 shared_informer.go:262] Caches are synced for garbage collector
I0831 11:58:22.216755       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0831 11:58:22.216936       1 shared_informer.go:262] Caches are synced for garbage collector
I0831 14:05:24.265759       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0831 14:05:24.265809       1 job_controller.go:498] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E0831 14:05:24.266504       1 tracking_utils.go:109] "deleting tracking annotation UID expectations" err="couldn't create key for object ingress-nginx/ingress-nginx-admission-create: could not find key for obj \"ingress-nginx/ingress-nginx-admission-create\"" job="ingress-nginx/ingress-nginx-admission-create"
E0831 14:05:24.266629       1 tracking_utils.go:109] "deleting tracking annotation UID expectations" err="couldn't create key for object ingress-nginx/ingress-nginx-admission-patch: could not find key for obj \"ingress-nginx/ingress-nginx-admission-patch\"" job="ingress-nginx/ingress-nginx-admission-patch"
I0831 14:05:29.367616       1 namespace_controller.go:185] Namespace has been deleted ingress-nginx
I0831 14:14:23.943258       1 namespace_controller.go:185] Namespace has been deleted ingress
I0831 14:19:19.917434       1 namespace_controller.go:185] Namespace has been deleted mysql-operator
I0831 14:21:45.422737       1 namespace_controller.go:185] Namespace has been deleted kube-node-lease

* 
* ==> kube-proxy [16a9df4c7673] <==
* I0831 14:35:36.274931       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0831 14:35:36.275038       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0831 14:35:36.275548       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0831 14:35:36.373141       1 server_others.go:206] "Using iptables Proxier"
I0831 14:35:36.373199       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0831 14:35:36.373207       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0831 14:35:36.373220       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0831 14:35:36.373301       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0831 14:35:36.373529       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0831 14:35:36.377668       1 server.go:661] "Version info" version="v1.24.3"
I0831 14:35:36.377684       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 14:35:36.379661       1 config.go:317] "Starting service config controller"
I0831 14:35:36.380515       1 shared_informer.go:255] Waiting for caches to sync for service config
I0831 14:35:36.383752       1 config.go:444] "Starting node config controller"
I0831 14:35:36.383774       1 shared_informer.go:255] Waiting for caches to sync for node config
I0831 14:35:36.383861       1 config.go:226] "Starting endpoint slice config controller"
I0831 14:35:36.383864       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0831 14:35:36.480868       1 shared_informer.go:262] Caches are synced for service config
I0831 14:35:36.484811       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0831 14:35:36.484866       1 shared_informer.go:262] Caches are synced for node config

* 
* ==> kube-proxy [e1a67fd4990c] <==
* I0831 11:58:16.333348       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0831 11:58:16.333435       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0831 11:58:16.333521       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0831 11:58:16.522578       1 server_others.go:206] "Using iptables Proxier"
I0831 11:58:16.522630       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0831 11:58:16.522639       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0831 11:58:16.522649       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0831 11:58:16.524211       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0831 11:58:16.524489       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0831 11:58:16.524652       1 server.go:661] "Version info" version="v1.24.3"
I0831 11:58:16.524709       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 11:58:16.544948       1 config.go:444] "Starting node config controller"
I0831 11:58:16.545980       1 shared_informer.go:255] Waiting for caches to sync for node config
I0831 11:58:16.554519       1 config.go:317] "Starting service config controller"
I0831 11:58:16.554553       1 shared_informer.go:255] Waiting for caches to sync for service config
I0831 11:58:16.554570       1 config.go:226] "Starting endpoint slice config controller"
I0831 11:58:16.554573       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0831 11:58:16.646637       1 shared_informer.go:262] Caches are synced for node config
I0831 11:58:16.654931       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0831 11:58:16.654989       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [36b75e5396a4] <==
* I0831 14:35:29.299812       1 serving.go:348] Generated self-signed cert in-memory
W0831 14:35:31.966701       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0831 14:35:31.966773       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0831 14:35:31.966786       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0831 14:35:31.966793       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0831 14:35:31.992026       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.3"
I0831 14:35:31.992132       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 14:35:32.016150       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0831 14:35:32.016309       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0831 14:35:32.023914       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 14:35:32.016330       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 14:35:32.125152       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [c355ad8627b6] <==
* I0831 11:58:01.313479       1 serving.go:348] Generated self-signed cert in-memory
W0831 11:58:03.353338       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0831 11:58:03.353413       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0831 11:58:03.353436       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0831 11:58:03.353443       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0831 11:58:03.381812       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.3"
I0831 11:58:03.381856       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 11:58:03.403002       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0831 11:58:03.403004       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0831 11:58:03.403608       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 11:58:03.403018       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 11:58:03.504309       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 14:22:01.382539       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0831 14:22:01.382788       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0831 14:22:01.382944       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"

* 
* ==> kubelet <==
* -- Logs begin at Wed 2022-08-31 14:35:13 UTC, end at Wed 2022-08-31 15:02:02 UTC. --
Aug 31 14:35:37 minikube kubelet[1113]: I0831 14:35:37.648159    1113 prober_manager.go:274] "Failed to trigger a manual run" probe="Readiness"
Aug 31 14:35:38 minikube kubelet[1113]: I0831 14:35:38.656821    1113 prober_manager.go:274] "Failed to trigger a manual run" probe="Readiness"
Aug 31 14:35:39 minikube kubelet[1113]: E0831 14:35:39.720509    1113 summary_sys_containers.go:83] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 31 14:35:49 minikube kubelet[1113]: E0831 14:35:49.840498    1113 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 31 14:35:49 minikube kubelet[1113]: E0831 14:35:49.840565    1113 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.386864    1113 topology_manager.go:200] "Topology Admit Handler"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.389725    1113 topology_manager.go:200] "Topology Admit Handler"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.566246    1113 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r5x4j\" (UniqueName: \"kubernetes.io/projected/039d9f50-ad2b-4bb5-95ea-9492bf68bbd4-kube-api-access-r5x4j\") pod \"ingress-nginx-admission-patch-phgst\" (UID: \"039d9f50-ad2b-4bb5-95ea-9492bf68bbd4\") " pod="ingress-nginx/ingress-nginx-admission-patch-phgst"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.566450    1113 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wxk57\" (UniqueName: \"kubernetes.io/projected/8890791f-49ca-4aee-bbd1-23a186d78add-kube-api-access-wxk57\") pod \"ingress-nginx-admission-create-cq75n\" (UID: \"8890791f-49ca-4aee-bbd1-23a186d78add\") " pod="ingress-nginx/ingress-nginx-admission-create-cq75n"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.568930    1113 topology_manager.go:200] "Topology Admit Handler"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.667389    1113 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qnmdr\" (UniqueName: \"kubernetes.io/projected/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-kube-api-access-qnmdr\") pod \"ingress-nginx-controller-755dfbfc65-5lkhx\" (UID: \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\") " pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-5lkhx"
Aug 31 14:35:50 minikube kubelet[1113]: I0831 14:35:50.667453    1113 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert\") pod \"ingress-nginx-controller-755dfbfc65-5lkhx\" (UID: \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\") " pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-5lkhx"
Aug 31 14:35:50 minikube kubelet[1113]: E0831 14:35:50.774357    1113 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 31 14:35:50 minikube kubelet[1113]: E0831 14:35:50.774495    1113 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert podName:835dd3d1-1e70-4ef7-88f6-563b43ec3d40 nodeName:}" failed. No retries permitted until 2022-08-31 14:35:51.2744719 +0000 UTC m=+26.365261501 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert") pod "ingress-nginx-controller-755dfbfc65-5lkhx" (UID: "835dd3d1-1e70-4ef7-88f6-563b43ec3d40") : secret "ingress-nginx-admission" not found
Aug 31 14:35:51 minikube kubelet[1113]: E0831 14:35:51.372603    1113 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 31 14:35:51 minikube kubelet[1113]: E0831 14:35:51.372738    1113 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert podName:835dd3d1-1e70-4ef7-88f6-563b43ec3d40 nodeName:}" failed. No retries permitted until 2022-08-31 14:35:52.3727009 +0000 UTC m=+27.463490601 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert") pod "ingress-nginx-controller-755dfbfc65-5lkhx" (UID: "835dd3d1-1e70-4ef7-88f6-563b43ec3d40") : secret "ingress-nginx-admission" not found
Aug 31 14:35:51 minikube kubelet[1113]: I0831 14:35:51.770725    1113 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="354baa2c915e23e9f8998890d7faf445e828ed0947031dc124e1b527b99e1a18"
Aug 31 14:35:51 minikube kubelet[1113]: I0831 14:35:51.773899    1113 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="e5032dbf45e40cd889b5684ce65836c19f906ad62fe56bcad4c586fb5be63f2a"
Aug 31 14:35:52 minikube kubelet[1113]: E0831 14:35:52.391219    1113 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 31 14:35:52 minikube kubelet[1113]: E0831 14:35:52.391342    1113 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert podName:835dd3d1-1e70-4ef7-88f6-563b43ec3d40 nodeName:}" failed. No retries permitted until 2022-08-31 14:35:54.3913122 +0000 UTC m=+29.482101801 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert") pod "ingress-nginx-controller-755dfbfc65-5lkhx" (UID: "835dd3d1-1e70-4ef7-88f6-563b43ec3d40") : secret "ingress-nginx-admission" not found
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.406097    1113 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-r5x4j\" (UniqueName: \"kubernetes.io/projected/039d9f50-ad2b-4bb5-95ea-9492bf68bbd4-kube-api-access-r5x4j\") pod \"039d9f50-ad2b-4bb5-95ea-9492bf68bbd4\" (UID: \"039d9f50-ad2b-4bb5-95ea-9492bf68bbd4\") "
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.425339    1113 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/039d9f50-ad2b-4bb5-95ea-9492bf68bbd4-kube-api-access-r5x4j" (OuterVolumeSpecName: "kube-api-access-r5x4j") pod "039d9f50-ad2b-4bb5-95ea-9492bf68bbd4" (UID: "039d9f50-ad2b-4bb5-95ea-9492bf68bbd4"). InnerVolumeSpecName "kube-api-access-r5x4j". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.507115    1113 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wxk57\" (UniqueName: \"kubernetes.io/projected/8890791f-49ca-4aee-bbd1-23a186d78add-kube-api-access-wxk57\") pod \"8890791f-49ca-4aee-bbd1-23a186d78add\" (UID: \"8890791f-49ca-4aee-bbd1-23a186d78add\") "
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.507299    1113 reconciler.go:312] "Volume detached for volume \"kube-api-access-r5x4j\" (UniqueName: \"kubernetes.io/projected/039d9f50-ad2b-4bb5-95ea-9492bf68bbd4-kube-api-access-r5x4j\") on node \"minikube\" DevicePath \"\""
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.528806    1113 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8890791f-49ca-4aee-bbd1-23a186d78add-kube-api-access-wxk57" (OuterVolumeSpecName: "kube-api-access-wxk57") pod "8890791f-49ca-4aee-bbd1-23a186d78add" (UID: "8890791f-49ca-4aee-bbd1-23a186d78add"). InnerVolumeSpecName "kube-api-access-wxk57". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.607751    1113 reconciler.go:312] "Volume detached for volume \"kube-api-access-wxk57\" (UniqueName: \"kubernetes.io/projected/8890791f-49ca-4aee-bbd1-23a186d78add-kube-api-access-wxk57\") on node \"minikube\" DevicePath \"\""
Aug 31 14:35:54 minikube kubelet[1113]: I0831 14:35:54.899575    1113 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="e5032dbf45e40cd889b5684ce65836c19f906ad62fe56bcad4c586fb5be63f2a"
Aug 31 14:35:55 minikube kubelet[1113]: I0831 14:35:55.192541    1113 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="02bec483fd98ce674ac5b2a4e8c5345e46688f0132ba90369fea6cea48e3024b"
Aug 31 14:35:55 minikube kubelet[1113]: I0831 14:35:55.200966    1113 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="354baa2c915e23e9f8998890d7faf445e828ed0947031dc124e1b527b99e1a18"
Aug 31 14:36:02 minikube kubelet[1113]: E0831 14:36:02.037369    1113 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 31 14:36:02 minikube kubelet[1113]: E0831 14:36:02.037424    1113 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 31 14:36:06 minikube kubelet[1113]: I0831 14:36:06.376627    1113 scope.go:110] "RemoveContainer" containerID="975376a436b55da2e7a524534155a2c419dcdd07363d039a62d63e404965e001"
Aug 31 14:36:06 minikube kubelet[1113]: I0831 14:36:06.376979    1113 scope.go:110] "RemoveContainer" containerID="e388a00ffca4b7e0378a6d53d2848d48d4fec635a6e575490f9293aef4a2715b"
Aug 31 14:36:06 minikube kubelet[1113]: E0831 14:36:06.377135    1113 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c8a46fbb-62ec-4604-a40f-f56a39954d00)\"" pod="kube-system/storage-provisioner" podUID=c8a46fbb-62ec-4604-a40f-f56a39954d00
Aug 31 14:36:14 minikube kubelet[1113]: E0831 14:36:14.353588    1113 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 31 14:36:14 minikube kubelet[1113]: E0831 14:36:14.353637    1113 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 31 14:36:19 minikube kubelet[1113]: I0831 14:36:19.142181    1113 scope.go:110] "RemoveContainer" containerID="e388a00ffca4b7e0378a6d53d2848d48d4fec635a6e575490f9293aef4a2715b"
Aug 31 14:38:57 minikube kubelet[1113]: E0831 14:38:57.389925    1113 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-controller-755dfbfc65-5lkhx.171074d8e38360c8", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-755dfbfc65-5lkhx", UID:"835dd3d1-1e70-4ef7-88f6-563b43ec3d40", APIVersion:"v1", ResourceVersion:"20194", FieldPath:"spec.containers{controller}"}, Reason:"Killing", Message:"Stopping container controller", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2022, time.August, 31, 14, 38, 57, 378509000, time.Local), LastTimestamp:time.Date(2022, time.August, 31, 14, 38, 57, 378509000, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "ingress-nginx-controller-755dfbfc65-5lkhx.171074d8e38360c8" is forbidden: unable to create new content in namespace ingress-nginx because it is being terminated' (will not retry!)
Aug 31 14:38:58 minikube kubelet[1113]: I0831 14:38:58.158725    1113 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=039d9f50-ad2b-4bb5-95ea-9492bf68bbd4 path="/var/lib/kubelet/pods/039d9f50-ad2b-4bb5-95ea-9492bf68bbd4/volumes"
Aug 31 14:38:58 minikube kubelet[1113]: I0831 14:38:58.159217    1113 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=8890791f-49ca-4aee-bbd1-23a186d78add path="/var/lib/kubelet/pods/8890791f-49ca-4aee-bbd1-23a186d78add/volumes"
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.086785    1113 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qnmdr\" (UniqueName: \"kubernetes.io/projected/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-kube-api-access-qnmdr\") pod \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\" (UID: \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\") "
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.086830    1113 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert\") pod \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\" (UID: \"835dd3d1-1e70-4ef7-88f6-563b43ec3d40\") "
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.101042    1113 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "835dd3d1-1e70-4ef7-88f6-563b43ec3d40" (UID: "835dd3d1-1e70-4ef7-88f6-563b43ec3d40"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.113042    1113 operation_generator.go:856] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-kube-api-access-qnmdr" (OuterVolumeSpecName: "kube-api-access-qnmdr") pod "835dd3d1-1e70-4ef7-88f6-563b43ec3d40" (UID: "835dd3d1-1e70-4ef7-88f6-563b43ec3d40"). InnerVolumeSpecName "kube-api-access-qnmdr". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.187100    1113 reconciler.go:312] "Volume detached for volume \"kube-api-access-qnmdr\" (UniqueName: \"kubernetes.io/projected/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-kube-api-access-qnmdr\") on node \"minikube\" DevicePath \"\""
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.187214    1113 reconciler.go:312] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/835dd3d1-1e70-4ef7-88f6-563b43ec3d40-webhook-cert\") on node \"minikube\" DevicePath \"\""
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.505633    1113 scope.go:110] "RemoveContainer" containerID="2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.523672    1113 scope.go:110] "RemoveContainer" containerID="2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:38:59 minikube kubelet[1113]: E0831 14:38:59.524920    1113 remote_runtime.go:578] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155" containerID="2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:38:59 minikube kubelet[1113]: I0831 14:38:59.525011    1113 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155} err="failed to get container status \"2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155\": rpc error: code = Unknown desc = Error: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.146587    1113 remote_runtime.go:484] "StopContainer from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155" containerID="2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.146689    1113 kuberuntime_container.go:727] "Container termination failed with gracePeriod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155" pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-5lkhx" podUID=835dd3d1-1e70-4ef7-88f6-563b43ec3d40 containerName="controller" containerID="docker://2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155" gracePeriod=1
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.146714    1113 kuberuntime_container.go:752] "Kill container failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155" pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-5lkhx" podUID=835dd3d1-1e70-4ef7-88f6-563b43ec3d40 containerName="controller" containerID={Type:docker ID:2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155}
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.148449    1113 kubelet.go:1759] failed to "KillContainer" for "controller" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155"
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.148586    1113 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155\"" pod="ingress-nginx/ingress-nginx-controller-755dfbfc65-5lkhx" podUID=835dd3d1-1e70-4ef7-88f6-563b43ec3d40
Aug 31 14:39:00 minikube kubelet[1113]: I0831 14:39:00.150632    1113 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=835dd3d1-1e70-4ef7-88f6-563b43ec3d40 path="/var/lib/kubelet/pods/835dd3d1-1e70-4ef7-88f6-563b43ec3d40/volumes"
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.206765    1113 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-controller-755dfbfc65-5lkhx.171074d8e38360c8", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-755dfbfc65-5lkhx", UID:"835dd3d1-1e70-4ef7-88f6-563b43ec3d40", APIVersion:"v1", ResourceVersion:"", FieldPath:"spec.containers{controller}"}, Reason:"Killing", Message:"Stopping container controller", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2022, time.August, 31, 14, 38, 57, 378509000, time.Local), LastTimestamp:time.Date(2022, time.August, 31, 14, 39, 0, 145337400, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "ingress-nginx" not found' (will not retry!)
Aug 31 14:39:00 minikube kubelet[1113]: E0831 14:39:00.261077    1113 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-controller-755dfbfc65-5lkhx.171074d9889cc8ec", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-755dfbfc65-5lkhx", UID:"835dd3d1-1e70-4ef7-88f6-563b43ec3d40", APIVersion:"v1", ResourceVersion:"", FieldPath:""}, Reason:"FailedKillPod", Message:"error killing pod: failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 2c2c73b1184d96121811bad7a3e0e4fe0dd6493422dcf5b793f9f2a091290155\"", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2022, time.August, 31, 14, 39, 0, 148414700, time.Local), LastTimestamp:time.Date(2022, time.August, 31, 14, 39, 0, 148414700, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "ingress-nginx" not found' (will not retry!)
Aug 31 14:39:25 minikube kubelet[1113]: I0831 14:39:25.972090    1113 scope.go:110] "RemoveContainer" containerID="1673f68524f54e44a365b275142464eb6ba3f3f794d79c4cf0476b31d36153ff"
Aug 31 14:39:26 minikube kubelet[1113]: I0831 14:39:26.014248    1113 scope.go:110] "RemoveContainer" containerID="bb5d506b06f0534ac83847d9b78a3c3c00cc726016ac977347dc863057414570"

* 
* ==> kubernetes-dashboard [26817c528491] <==
* 2022/08/31 11:59:05 Using namespace: kubernetes-dashboard
2022/08/31 11:59:05 Using in-cluster config to connect to apiserver
2022/08/31 11:59:05 Using secret token for csrf signing
2022/08/31 11:59:05 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/08/31 11:59:05 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2022/08/31 11:59:05 Successful initial request to the apiserver, version: v1.24.3
2022/08/31 11:59:05 Generating JWE encryption key
2022/08/31 11:59:06 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/08/31 11:59:06 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/08/31 11:59:06 Initializing JWE encryption key from synchronized object
2022/08/31 11:59:06 Creating in-cluster Sidecar client
2022/08/31 11:59:06 Serving insecurely on HTTP port: 9090
2022/08/31 11:59:06 Successful request to sidecar
2022/08/31 11:59:05 Starting overwatch

* 
* ==> kubernetes-dashboard [35c9f6417194] <==
* 2022/08/31 14:35:37 Using namespace: kubernetes-dashboard
2022/08/31 14:35:37 Using in-cluster config to connect to apiserver
2022/08/31 14:35:37 Using secret token for csrf signing
2022/08/31 14:35:37 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/08/31 14:35:37 Successful initial request to the apiserver, version: v1.24.3
2022/08/31 14:35:37 Generating JWE encryption key
2022/08/31 14:35:37 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/08/31 14:35:37 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/08/31 14:35:37 Initializing JWE encryption key from synchronized object
2022/08/31 14:35:37 Creating in-cluster Sidecar client
2022/08/31 14:35:37 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/08/31 14:35:37 Serving insecurely on HTTP port: 9090
2022/08/31 14:36:07 Successful request to sidecar
2022/08/31 14:35:37 Starting overwatch

* 
* ==> storage-provisioner [4c7af58efd52] <==
* I0831 14:36:19.440456       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0831 14:36:19.467937       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0831 14:36:19.468524       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0831 14:36:36.944602       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0831 14:36:36.945195       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"13ce8729-b83e-43cc-91ca-69c088ae1379", APIVersion:"v1", ResourceVersion:"20276", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f5c11782-3a21-42bd-a876-794c7f5e7288 became leader
I0831 14:36:36.945440       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f5c11782-3a21-42bd-a876-794c7f5e7288!
I0831 14:36:37.047585       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f5c11782-3a21-42bd-a876-794c7f5e7288!

* 
* ==> storage-provisioner [e388a00ffca4] <==
* I0831 14:35:35.389243       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0831 14:36:05.431597       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

